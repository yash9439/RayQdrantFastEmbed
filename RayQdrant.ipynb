{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Docker Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo docker pull qdrant/qdrant\n",
    "\n",
    "sudo docker run -p 6333:6333 -p 6334:6334 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n",
    "    qdrant/qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        extracted_text += page.extract_text()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_text_from_pdfs_in_directory(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(directory, filename)\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "            txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            txt_filepath = os.path.join(directory, txt_filename)\n",
    "            with open(txt_filepath, \"w\") as txt_file:\n",
    "                txt_file.write(extracted_text)\n",
    "\n",
    "# Specify the directory containing PDF files\n",
    "directory_path = \"Docs/\"\n",
    "\n",
    "# Extract text from PDFs in the directory and save as text files\n",
    "extract_text_from_pdfs_in_directory(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Claude 3 Model Family: Opus, Sonnet, Haiku\\nAnthropic\\nAbstract\\nWe introduce Claude 3, a new family of large multimodal models – Claude 3 Opus , our\\nmost capable offering, Claude 3 Sonnet , which provides a combination of skills and speed,\\nandClaude 3 Haiku , our fastest and least expensive model.', 'All new models have vision\\ncapabilities that enable them to process and analyze image data.', 'The Claude 3 family\\ndemonstrates strong performance across benchmark evaluations and sets a new standard on\\nmeasures of reasoning, math, and coding.', 'Claude 3 Opus achieves state-of-the-art results\\non evaluations like GPQA [1], MMLU [2], MMMU [3] and many more.', 'Claude 3 Haiku\\nperforms as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and\\nOpus significantly outperform it.', 'Additionally, these models exhibit improved fluency in\\nnon-English languages, making them more versatile for a global audience.', 'In this report,\\nwe provide an in-depth analysis of our evaluations, focusing on core capabilities, safety,\\nsocietal impacts, and the catastrophic risk assessments we committed to in our Responsible\\nScaling Policy [5].', '1 Introduction\\nThis model card introduces the Claude 3 family of models, which set new industry benchmarks across rea-\\nsoning, math, coding, multi-lingual understanding, and vision quality.', 'Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and\\nConstitutional AI [6].', 'These models were trained using hardware from Amazon Web Services (AWS) and\\nGoogle Cloud Platform (GCP), with core frameworks including PyTorch [7], JAX [8], and Triton [9].']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "directory_path = \"Docs\"\n",
    "\n",
    "# List all .txt files in the directory\n",
    "txt_files = [file for file in os.listdir(directory_path) if file.endswith('.txt')]\n",
    "\n",
    "# List to store sentences from all files\n",
    "all_sentences = []\n",
    "\n",
    "# Read each text file, split into sentences, and store\n",
    "for txt_file in txt_files:\n",
    "    file_path = os.path.join(directory_path, txt_file)\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        all_sentences.extend(sentences)\n",
    "\n",
    "# Print the first few sentences as an example\n",
    "print(all_sentences[:10])  # Print first 10 sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Embedding for the text using FastEmbed and Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252M/252M [00:18<00:00, 13.9MiB/s] \n",
      "2024-03-12 18:54:01,171\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to generate embeddings with Ray Distributed Computing: 197.39826798439026 seconds\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from fastembed import TextEmbedding\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "TextEmbedding(model_name=\"BAAI/bge-base-en\", cache_dir=\"./embeddings\")\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "@ray.remote\n",
    "class EmbeddingWorker:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = TextEmbedding(model_name=\"BAAI/bge-base-en\", cache_dir=\"./embeddings\")\n",
    "\n",
    "    def embed_documents(self, documents):\n",
    "        embeddings = []\n",
    "        for document in documents:\n",
    "            embeddings.append(np.array(list(self.embedding_model.embed([document]))))\n",
    "        return embeddings\n",
    "\n",
    "# Define the number of workers\n",
    "num_workers = 4  # Adjust this according to your resources\n",
    "documents = all_sentences\n",
    "\n",
    "# Split documents into chunks for each worker\n",
    "chunk_size = len(documents) // num_workers\n",
    "document_chunks = [documents[i:i+chunk_size] for i in range(0, len(documents), chunk_size)]\n",
    "\n",
    "# Start the workers\n",
    "embedding_workers = [EmbeddingWorker.remote() for _ in range(num_workers)]\n",
    "\n",
    "# Perform embedding generation in parallel\n",
    "start_time = time.time()\n",
    "embedding_tasks = [worker.embed_documents.remote(chunk) for worker, chunk in zip(embedding_workers, document_chunks)]\n",
    "embeddings = ray.get(embedding_tasks)\n",
    "end_time = time.time()\n",
    "\n",
    "# Flatten the embeddings list\n",
    "embeddings = [embedding for sublist in embeddings for embedding in sublist]\n",
    "\n",
    "print(\"Time taken to generate embeddings with Ray Distributed Computing:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()\n",
    "embeddings = [sublist[0] for sublist in embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Qdrant-Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "# client = QdrantClient(path=\"./DB\")\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "collection_name = 'fastembed_collection'\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Embedding to Qdrant Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=idx,\n",
    "            vector=vector.tolist(),\n",
    "            payload={\"color\": \"red\", \"rand_number\": idx % 10}\n",
    "        )\n",
    "        for idx, vector in enumerate(embeddings)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Query Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-12 18:57:28,462\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to generate embeddings with Ray Distributed Computing: 1.8792011737823486 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from fastembed import TextEmbedding\n",
    "import ray \n",
    "import numpy as np\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "@ray.remote\n",
    "class EmbeddingWorker:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = TextEmbedding(model_name=\"BAAI/bge-base-en\", cache_dir=\"./embeddings\")\n",
    "\n",
    "    def embed_query(self, documents):\n",
    "        embeddings = []\n",
    "        for document in documents:\n",
    "            embeddings.append(np.array(list(self.embedding_model.embed([document]))))\n",
    "        return embeddings\n",
    "\n",
    "# Define the number of workers\n",
    "num_workers = 2  # Adjust this according to your resources\n",
    "query = [\"Can AI Models be hacked?\",\"How to secure AI models?\"]\n",
    "\n",
    "# Split query into chunks for each worker\n",
    "chunk_size = len(query) // num_workers\n",
    "document_chunks = [query[i:i+chunk_size] for i in range(0, len(query), chunk_size)]\n",
    "\n",
    "# Start the workers\n",
    "embedding_workers = [EmbeddingWorker.remote() for _ in range(num_workers)]\n",
    "\n",
    "# Perform embedding generation in parallel\n",
    "start_time = time.time()\n",
    "embedding_tasks = [worker.embed_query.remote(chunk) for worker, chunk in zip(embedding_workers, document_chunks)]\n",
    "embeddings = ray.get(embedding_tasks)\n",
    "end_time = time.time()\n",
    "\n",
    "# Flatten the embeddings list\n",
    "embeddings = [embedding for sublist in embeddings for embedding in sublist]\n",
    "\n",
    "print(\"Time taken to generate embeddings with Ray Distributed Computing:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()\n",
    "query_embeddings = [sublist[0] for sublist in embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Top limit number of similar sentences to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScoredPoint(id=4153, version=194, score=0.8726248, payload={'color': 'red', 'rand_number': 3}, vector=None, shard_key=None), ScoredPoint(id=6577, version=308, score=0.8577993, payload={'color': 'red', 'rand_number': 7}, vector=None, shard_key=None), ScoredPoint(id=6974, version=326, score=0.8554286, payload={'color': 'red', 'rand_number': 4}, vector=None, shard_key=None), ScoredPoint(id=6507, version=305, score=0.8542422, payload={'color': 'red', 'rand_number': 7}, vector=None, shard_key=None), ScoredPoint(id=3234, version=152, score=0.8520849, payload={'color': 'red', 'rand_number': 4}, vector=None, shard_key=None), ScoredPoint(id=3249, version=152, score=0.85169685, payload={'color': 'red', 'rand_number': 9}, vector=None, shard_key=None), ScoredPoint(id=6921, version=326, score=0.85126793, payload={'color': 'red', 'rand_number': 1}, vector=None, shard_key=None), ScoredPoint(id=3172, version=149, score=0.85126793, payload={'color': 'red', 'rand_number': 2}, vector=None, shard_key=None)]\n",
      "In Figure 10, we show one exploit using adversarial system messages (which are intended to help\n",
      "set the behavior of the model).\n",
      "-------------\n",
      "Generative ai prohibited use policy, 2023a.\n",
      "-------------\n",
      "Ethical and social risks of harm from language models.\n",
      "-------------\n",
      "Build it break it ﬁx it for dialogue safety: Robustness from\n",
      "adversarial human attack.\n",
      "-------------\n",
      "TruthfulQA: Measuring how models mimic\n",
      "human falsehoods.\n",
      "-------------\n",
      "Is power-seeking AI an existential risk?\n",
      "-------------\n",
      "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language\n",
      "models.\n",
      "-------------\n",
      "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\n",
      "-------------\n",
      "=====================================\n",
      "[ScoredPoint(id=376, version=17, score=0.873445, payload={'color': 'red', 'rand_number': 6}, vector=None, shard_key=None), ScoredPoint(id=6577, version=308, score=0.87057495, payload={'color': 'red', 'rand_number': 7}, vector=None, shard_key=None), ScoredPoint(id=6974, version=326, score=0.8640828, payload={'color': 'red', 'rand_number': 4}, vector=None, shard_key=None), ScoredPoint(id=6507, version=305, score=0.8625968, payload={'color': 'red', 'rand_number': 7}, vector=None, shard_key=None), ScoredPoint(id=3243, version=152, score=0.8585173, payload={'color': 'red', 'rand_number': 3}, vector=None, shard_key=None), ScoredPoint(id=4237, version=200, score=0.8579695, payload={'color': 'red', 'rand_number': 7}, vector=None, shard_key=None), ScoredPoint(id=3301, version=155, score=0.85755396, payload={'color': 'red', 'rand_number': 1}, vector=None, shard_key=None), ScoredPoint(id=6849, version=323, score=0.8561846, payload={'color': 'red', 'rand_number': 9}, vector=None, shard_key=None)]\n",
      "We harden security against opportunistic attackers for all copies of\n",
      "Claude 3 model weights.\n",
      "-------------\n",
      "Generative ai prohibited use policy, 2023a.\n",
      "-------------\n",
      "Ethical and social risks of harm from language models.\n",
      "-------------\n",
      "Build it break it ﬁx it for dialogue safety: Robustness from\n",
      "adversarial human attack.\n",
      "-------------\n",
      "OpenAI: How should AI systems behave, and who should decide?, 2023.\n",
      "-------------\n",
      "73[38]OpenAI, “How should AI systems behave, and who should decide?.\n",
      "-------------\n",
      "System Cards, a new\n",
      "resource for understanding how AI systems work.\n",
      "-------------\n",
      "Roberts, A., Raffel, C., and Shazeer, N. How much knowledge can you pack into the parameters of a language model?\n",
      "-------------\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "collection_name = 'fastembed_collection'\n",
    "for query_embedding in query_embeddings:\n",
    "    query_vector: List[np.ndarray] = list(query_embedding)\n",
    "    hits = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=8 \n",
    "    )\n",
    "    print(hits)\n",
    "\n",
    "    for i in range(8):\n",
    "        print(all_sentences[hits[i].id])\n",
    "        print(\"-------------\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
